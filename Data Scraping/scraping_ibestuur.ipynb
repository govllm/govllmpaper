{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7290f1d80b22dedc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scraping iBestuur articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4242678a68f511",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d099b23dd7966d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:00:09.118518Z",
     "start_time": "2024-04-02T12:00:08.509942Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205674ff12cac68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 1: Fetch sitemap urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa399e0fe02bc9f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:00:09.122366Z",
     "start_time": "2024-04-02T12:00:09.119756Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create directory\n",
    "Path('../Data/iBestuur').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6361feddb2412fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:00:09.126920Z",
     "start_time": "2024-04-02T12:00:09.123173Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_sitemap_urls(sitemap_index_url):\n",
    "    \"\"\"Fetch a sitemap and return a list of URLs.\"\"\"\n",
    "    response = requests.get(sitemap_index_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'xml')\n",
    "        sitemap_urls = [element.text for element in soup.find_all('loc')]\n",
    "        return sitemap_urls\n",
    "    else:\n",
    "        print('Failed to retrieve the XML Sitemap Index file.')\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368fc7faa670dd7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 2: Fetch all urls from the different sitemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af5df38ca7c8c6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:00:09.131701Z",
     "start_time": "2024-04-02T12:00:09.128020Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_urls(sitemap_urls):\n",
    "    \"\"\"Fetch all URLs from a list of sitemaps.\"\"\"\n",
    "    all_urls = []\n",
    "\n",
    "    for sitemap_url in sitemap_urls:\n",
    "        response = requests.get(sitemap_url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'xml')\n",
    "            urls = [element.text for element in soup.find_all('loc')]\n",
    "            all_urls.extend(urls)\n",
    "        time.sleep(0.1)  # Be respectful by not hammering the server\n",
    "\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e19e0609a9312e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 3: Fetch the content from the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9402ee9153bb0deb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:00:09.140043Z",
     "start_time": "2024-04-02T12:00:09.134048Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_content_and_write_to_csv(urls, csv_file_path):\n",
    "    \"\"\"Fetch the content from a list of URLs.\"\"\"\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Check if the file is empty\n",
    "        file.seek(0, 2) # Go to the end of the file\n",
    "        if file.tell() == 0:  # If the file is empty, write the header\n",
    "            writer.writerow(['title', 'subtitle', 'author', 'date', 'content', 'url'])\n",
    "\n",
    "        for url in tqdm(urls, desc='Scraping Articles', unit=' article'):\n",
    "            try:\n",
    "                response = requests.get(url, allow_redirects=True)\n",
    "            except requests.exceptions.TooManyRedirects:\n",
    "                with open('../Data/iBestuur/problematic_urls.txt', 'a') as f:\n",
    "                    f.write(f'{url}\\n')\n",
    "                continue # Skip to the next URL if too many redirects occur\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                script_tag = soup.find('script', id='smg_data')\n",
    "                data = json.loads(script_tag.string) if script_tag else {}\n",
    "                \n",
    "                title = soup.find('h1').text.strip() if soup.find('h1') else 'NaN'\n",
    "                \n",
    "                author = data.get('author')\n",
    "                if not author:\n",
    "                    author = data.get('partner', 'NaN')\n",
    "                \n",
    "                date = data.get('first_published_at', 'NaN')\n",
    "                \n",
    "                content = ' '.join([p.text for p in soup.find_all('p')])\n",
    "                \n",
    "                if content:\n",
    "                    writer.writerow([title, 'NaN', author, date, content, url])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f372dc9f7ba0f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 4: Exclude unnecessary URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c623997f876a0bb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:00:09.145373Z",
     "start_time": "2024-04-02T12:00:09.141139Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exclude_urls(urls):\n",
    "    \"\"\"Exclude URLs that are not articles.\"\"\"\n",
    "    # Exclude pictures (URLs that end with .jpg)\n",
    "    all_urls = [url for url in urls if not url.endswith('.jpg')]\n",
    "    \n",
    "    # Exclude already scraped urls\n",
    "    try:\n",
    "        df = pd.read_csv('../Data/iBestuur/ibestuur_articles.csv')\n",
    "        scraped_urls = df['url'].tolist()\n",
    "    except FileNotFoundError:\n",
    "        scraped_urls = []\n",
    "        \n",
    "    urls_to_scrape = [url for url in all_urls if url not in scraped_urls]\n",
    "    \n",
    "    return urls_to_scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b161832e0088f78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 5: Setting up the scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50eadcb269f645f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:00:09.150567Z",
     "start_time": "2024-04-02T12:00:09.146461Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Step 1: Fetch sitemap URLs\n",
    "    sitemap_index_url = 'https://ibestuur.nl/sitemap_index.xml'\n",
    "    sitemap_urls = fetch_sitemap_urls(sitemap_index_url)\n",
    "    print(f\"Total number of sitemaps: {len(sitemap_urls)}\")\n",
    "    \n",
    "    # Exclude unnecessary sitemaps (NOW ONLY ARTICLES ARE INCLUDED !!!)\n",
    "    sitemap_urls = [url for url in sitemap_urls if 'artikel' in url]\n",
    "    print(f\"Total number of sitemaps with articles: {len(sitemap_urls)}\")\n",
    "\n",
    "    # Step 2: Fetch all URLs from the different sitemaps\n",
    "    all_urls = fetch_urls(sitemap_urls)\n",
    "    \n",
    "    # Step 4: Exclude unnecessary URLs\n",
    "    all_urls = exclude_urls(all_urls)\n",
    "    print(f\"Total number of articles that will be scraped: {len(all_urls)}\")\n",
    "\n",
    "    # Step 3: Fetch the content from the URLs\n",
    "    csv_file_path = '../Data/iBestuur/ibestuur_articles.csv'\n",
    "    fetch_content_and_write_to_csv(all_urls, csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5031b3e4ca54e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 6: Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fca2c0b8285dbe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:14:11.063372Z",
     "start_time": "2024-04-02T12:00:09.151777Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sitemaps: 36\n",
      "Total number of sitemaps with articles: 6\n",
      "Total number of articles that will be scraped: 1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Articles: 100%|██████████| 1281/1281 [13:37<00:00,  1.57 article/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd76e63abbdf561d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:14:11.066967Z",
     "start_time": "2024-04-02T12:14:11.065209Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4adcce336bcc84b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:14:11.112998Z",
     "start_time": "2024-04-02T12:14:11.067976Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/iBestuur/ibestuur_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a61ce76f9947361",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:14:11.125760Z",
     "start_time": "2024-04-02T12:14:11.113893Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(�HB\\��&lt;�\u000eO�Û���&lt;پ��l�3����\u0012��\\t*pZ�\u0013\fe~��6�(�...</td>\n",
       "      <td>https://ibestuur-uploads.storage.googleapis.co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title  subtitle author date  \\\n",
       "11   NaN       NaN    NaN  NaN   \n",
       "\n",
       "                                              content  \\\n",
       "11  (�HB\\��<�\u000eO�Û���<پ��l�3����\u0012��\\t*pZ�\u0013\n",
       "e~��6�(�...   \n",
       "\n",
       "                                                  url  \n",
       "11  https://ibestuur-uploads.storage.googleapis.co...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select rows with NaN values in the 'author' column\n",
    "df[df['author'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21c3085abb48a4c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:14:11.129763Z",
     "start_time": "2024-04-02T12:14:11.126670Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9deb46c81e0c8b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:14:11.132807Z",
     "start_time": "2024-04-02T12:14:11.130551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
